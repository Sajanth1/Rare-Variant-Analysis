# -*- coding: utf-8 -*-
"""VEP.show_variants.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vpimGTCoEC1Mn0aaUKlrXxWTSNxbWe8a
"""

import pandas as pd
import numpy as np
import sys

from scipy.stats import chi2_contingency
from scipy.stats import fisher_exact
from scipy.stats import norm

# from statsmodels.stats.meta_analysis import combine_effects

# Accept cohort, case_total, and control_total from command line
cohort = sys.argv[1]
case_total = int(sys.argv[2])
control_total = int(sys.argv[3])

"""#Process annotations"""

VEP_path = f"annotation/{cohort}.VEPannotated_split.csv"
freq_path = f"annotation/{cohort}_MAF.assoc.fisher"
counts_path = f"annotation/{cohort}.assoc.fisher"

VEP_df = pd.read_csv(VEP_path, sep="\t", low_memory=False)


# Reformat Functions


def extract0(col):
    parts = str(col).split(":", 1)
    if len(parts) > 1:
        return parts[0]
    else:
        return col  # Or return None, or any other suitable default value


def extract1(col):
    parts = str(col).split(":", 1)
    if len(parts) > 1:
        return parts[1]
    else:
        return col


# Actual Reformat

VEP_df["Chromosome"] = VEP_df["Location"].apply(extract0)
VEP_df["Location"] = VEP_df["Location"].apply(extract1)

VEP_df["HGVSc"] = VEP_df["HGVSc"].apply(extract1)
VEP_df["HGVSp"] = VEP_df["HGVSp"].apply(extract1)

VEP_df["Consequence"] = VEP_df["Consequence"].str.replace("_variant", "")

# Filter & Reorder
cols_req = [
    "name",
    "Chromosome",
    "Location",
    "Ref",
    "Alt",
    "HGVSc",
    "HGVSp",
    "Tag",
    "Consequence",
    "CADD_PHRED",
    "AM_score",
    "ClinVar",
    "gnomADg_AF",
    "gnomADg_ASJ_AF",
    "gnomADg_NFE_AF",
]

VEP_df = VEP_df[cols_req]

# Rename
header_map = {
    "Ref": "Reference Allele",
    "Alt": "Alternate Allele",
    "HGVSc": "cDNA",
    "HGVSp": "Amino Acid",
    "CADD_PHRED": "CADD v1.7 Phred Score",
    "AM_score": "AlphaMissense Score",
    "ClinVar": "Clinical Significance (ClinVar)",
    "gnomADg_AF": "gnomADg Allele Frequency",
    "gnomADg_ASJ_AF": "gnomADg AF Ashkenazi",
    "gnomADg_NFE_AF": "gnomADg AF Non-Finnish European",
}

VEP_df.rename(columns=header_map, inplace=True)


"""#Process freqs & counts, then Merge"""

# Read freq and counts files
freq_df = pd.read_csv(freq_path, sep=r"\s+")
counts_df = pd.read_csv(counts_path, sep=r"\s+")

# Format data types to match
freq_df["SNP"] = freq_df["SNP"].astype(str)
counts_df["SNP"] = counts_df["SNP"].astype(str)

# merge the two files
MAF_and_counts_df = pd.merge(
    counts_df[["CHR", "SNP", "BP", "A1", "A2", "C_A", "C_U"]],
    freq_df[["SNP", "F_A", "F_U"]],
    on="SNP",
)


# Based on Tag, merge the VEP and MAF_and_counts dataframes
for col in ["A1", "A2"]:
    MAF_and_counts_df[col] = MAF_and_counts_df[col].astype(str)

VEP_df = VEP_df.astype({"Reference Allele": str, "Alternate Allele": str})

# Cannot merge based on rsID because it's very common for rsIDs
# to appear multiple times in a dataset, especially for multi-allelic sites.

merged_df = pd.merge(VEP_df, MAF_and_counts_df, left_on=["Tag"], right_on=["SNP"]).drop(
    columns=["CHR", "SNP", "BP", "A1", "A2"]
)


"""Unmatched troubleshooting"""

# Count number of rows
print(f"Number of rows: {len(merged_df)}")

# Unmatched rows from MAF_and_counts_df
unmatched = merged_df[~VEP_df["Tag"].isin(merged_df["Tag"])]
print(f"Unmatched row count: {len(unmatched)}")


"""# Reformat"""

# Rename new col
header_map2 = {
    "C_A": "N (Case)",
    "C_U": "N (Control)",
    "F_A": "MAF (Case)",
    "F_U": "MAF (Control)",
}

merged_df = merged_df.rename(columns=header_map2)


# Remove variants where both N (Case) and N (Control) are 0

excluded_df = merged_df[(merged_df["N (Case)"] == 0) & (merged_df["N (Control)"] == 0)]
print(f"Number of excluded rows: {len(excluded_df)}")


merged_df = merged_df[~merged_df.index.isin(excluded_df.index)]

# Re-order

last_4_cols = merged_df.columns[-4:]
other_cols = merged_df.columns[:-4]

# Insert the last 4 columns at the 9th position
reordered_cols = list(other_cols[:9]) + list(last_4_cols) + list(other_cols[9:])

# Reorder the DataFrame
ordered_merged_df = merged_df[reordered_cols]


"""#Odds Ratio Calculation"""


def calculate_confidence_interval(a, b, c, d):
    """
    Calculate 95% CI for the Odds Ratio using the normal approximation.
    a = case_count
    b = case_total - case_count
    c = control_count
    d = control_total - control_count
    """
    # Calculate OR
    if a == 0 or b == 0 or c == 0 or d == 0:
        return np.nan, np.nan
    or_val = (a * d) / (b * c)
    # Check for valid denominators before sqrt
    if a <= 0 or b <= 0 or c <= 0 or d <= 0:
        return np.nan, np.nan
    se = np.sqrt((1 / a) + (1 / b) + (1 / c) + (1 / d))
    log_or = np.log(or_val)
    ci_lower = np.exp(log_or - 1.96 * se)
    ci_upper = np.exp(log_or + 1.96 * se)
    return ci_lower, ci_upper


def calculate_p_value_from_or(a, b, c, d):
    """
    Calculate p-value based on the Odds Ratio and its standard error.
    """
    if a == 0 or b == 0 or c == 0 or d == 0:
        return np.nan
    or_val = (a * d) / (b * c)
    # Check for valid denominators before sqrt
    if a <= 0 or b <= 0 or c <= 0 or d <= 0:
        return np.nan
    se_log_or = np.sqrt((1 / a) + (1 / b) + (1 / c) + (1 / d))
    z = np.log(or_val) / se_log_or
    p_value = 2 * (1 - norm.cdf(abs(z)))
    return p_value


def calculate_odds_ratio_no_adjust(data, case_total, control_total):
    """
    Calculate Odds Ratios without continuity correction for a specific cohort.
    Use OR-based p-value calculation for consistency with OR and CI.
    """
    odds_ratios = []
    p_values = []
    ci_lowers = []
    ci_uppers = []

    for _, row in data.iterrows():
        case_count = row["N (Case)"]
        control_count = row["N (Control)"]

        a = case_count
        b = case_total - case_count
        c = control_count
        d = control_total - control_count

        try:
            # Calculate OR
            if a == 0 or b == 0 or c == 0 or d == 0:
                odds_ratio = np.nan
                p = np.nan
                ci_lower, ci_upper = np.nan, np.nan
            else:
                odds_ratio = (a * d) / (b * c)

                # Use OR-based p-value calculation
                p = calculate_p_value_from_or(a, b, c, d)

                # Calculate CI
                ci_lower, ci_upper = calculate_confidence_interval(a, b, c, d)

        except ZeroDivisionError:
            odds_ratio, p = np.nan, np.nan
            ci_lower, ci_upper = np.nan, np.nan

        odds_ratios.append(odds_ratio)
        p_values.append(p)
        ci_lowers.append(ci_lower)
        ci_uppers.append(ci_upper)

    data = data.copy()
    data.loc[:, "OR"] = odds_ratios
    data.loc[:, "P-value"] = p_values
    data.loc[:, "OR 95% CI Lower"] = ci_lowers
    data.loc[:, "OR 95% CI Upper"] = ci_uppers
    return data


def calculate_odds_ratio_ratio_adjusted(data, case_total, control_total):
    """
    Calculate Odds Ratios with ratio-based continuity correction for a specific cohort.
    Use OR-based p-value calculation for consistency with OR and CI.
    """
    odds_ratios = []
    p_values = []
    ci_lowers = []
    ci_uppers = []

    for _, row in data.iterrows():
        case_count = row["N (Case)"]
        control_count = row["N (Control)"]

        # Apply ratio-based continuity correction
        a = case_count + 0.5
        b = (case_total - case_count) + 0.5
        c = control_count + 0.5
        d = (control_total - control_count) + 0.5

        try:
            # Calculate OR
            if a == 0 or b == 0 or c == 0 or d == 0:
                odds_ratio = np.nan
                p = np.nan
                ci_lower, ci_upper = np.nan, np.nan
            else:
                odds_ratio = (a * d) / (b * c)

                # Use OR-based p-value calculation
                p = calculate_p_value_from_or(a, b, c, d)

                # Calculate CI using corrected counts
                ci_lower, ci_upper = calculate_confidence_interval(a, b, c, d)

        except ZeroDivisionError:
            odds_ratio, p = np.nan, np.nan
            ci_lower, ci_upper = np.nan, np.nan

        odds_ratios.append(odds_ratio)
        p_values.append(p)
        ci_lowers.append(ci_lower)
        ci_uppers.append(ci_upper)

    data = data.copy()
    data.loc[:, "OR (Continuity Corrected)"] = odds_ratios
    data.loc[:, "P-value (Continuity Corrected)"] = p_values
    data.loc[:, "OR (Continuity Corrected) 95% CI Lower"] = ci_lowers
    data.loc[:, "OR (Continuity Corrected) 95% CI Upper"] = ci_uppers
    return data


# Calculate corrected and non-corrected ORs and P-values
adjusted_OR_df = calculate_odds_ratio_ratio_adjusted(
    ordered_merged_df, case_total, control_total
)
final_df = calculate_odds_ratio_no_adjust(adjusted_OR_df, case_total, control_total)


"""#Reformat and Save"""

# Re-order
appended_cols = final_df.columns[-8:]
initial_cols = final_df.columns[:-8]

# Insert after MAF (Control)
maf_index = final_df.columns.get_loc("MAF (Control)")

reordered_cols = (
    list(initial_cols[: maf_index + 1])
    + list(appended_cols)
    + list(initial_cols[maf_index + 1 :])
)
ordered_final_df = final_df[reordered_cols].drop(columns=["Tag"])

# save into excel file
ordered_final_df.to_csv(
    f"annotation/{cohort}.single_variants.csv", sep=",", index=False
)
